{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP767 - MDP.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "HVMlXzAUOHg9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Assignment\n",
        "\n",
        "## St√©phanie Larocque and Philip Paquette\n",
        "\n",
        "Link to Colab: https://drive.google.com/file/d/1Tk1htsWcckFMg5Xa7VVR8cn6X_VEQUbC/view?usp=sharing"
      ]
    },
    {
      "metadata": {
        "id": "9EqR6ZrcHnNW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question 1\n",
        "##  Show that the Bellman optimality operator is a contraction mapping\n",
        "\n",
        "For a value function $v$, The Bellman optimality operator, noted as $B$, is \n",
        "$$\\text{Bellman}(v(s)) = Bv(s) = \\max_a \\sum\\limits_{s', r} p(s',r| s, a) [r + \\gamma v(s')].$$\n",
        "\n",
        "Therefore, proving that the Bellman optimality operator is a contracting mapping is proving that, for two arbitrary value functions $v_1$ and $v_2$, there exists a constant $0\\leq k <1$ such that\n",
        "$$||Bv_1(s)-Bv_2(s)||\\leq k\\cdot ||v_1(s)-v_2(s)||.$$\n",
        "It means that the distance between two policies $v_1$ and $v_2$ becomes smaller each time the Bellman optimality operator is applied to them (i.e. that they converge to the same fixed point, by Banach fixed-point theorem). We will use the infinite norm which is, on a finite set $\\mathcal{X}$:\n",
        "$$||f||_\\infty = \\max_{x\\in \\mathcal{X}} |f(x)|$$\n",
        "We will also use the following properties, always on a finite set $\\mathcal{X}$:\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "\\text{Property 1.  }&|\\max_x f(x)|\\leq \\max_x |f(x)|\\\\\n",
        "\\text{Property 2.  }&|\\max_x f(x) - \\max_x g(x)|\\leq \\max_x|f(x)-g(x)|\\\\\n",
        "\\text{Property 3.  }&|\\sum_x p(x)f(x) |\\leq \\sum_x |p(x)f(x)|\\leq \\max_x |f(x)|, \\text{ where $p(x)$ is a probability}\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "\n",
        "Therefore, we have\n",
        "\n",
        "\\begin{align*}\n",
        "||Bv_1(s)-Bv_2(s)||_\\infty\n",
        "  & = \\max_s |Bv_1(s) - Bv_2(s)| &&\\text{By definition of infinite norm}\\\\\n",
        "  & = \\max_s \\left| \\max_a \\sum\\limits_{s', r} p(s',r| s, a) \\big[r + \\gamma v_1(s')\\big] -  \\max_a \\sum\\limits_{s', r} p(s',r| s, a) \\big[r + \\gamma v_2(s')\\big]\\right|\\\\\n",
        "  & \\leq \\max_s \\max_a \\left| \\sum\\limits_{s', r} p(s',r| s, a) \\big[r + \\gamma v_1(s')-r - \\gamma v_2(s')\\big]\\right| &&\\text{Property 2.}\\\\\n",
        "  & = \\gamma \\max_s \\max_a \\left| \\sum\\limits_{s', r} p(s',r| s, a) \\big[v_1(s')- v_2(s')\\big]\\right|\\\\\n",
        "  & \\leq \\gamma \\max_s \\max_a \\max_{s'} |v_1(s') - v_2(s')|&&\\text{Property 3. }\\\\\n",
        "  & = \\gamma \\max_{s'}|v_1(s')-v_2(s')|&&\\text{Remove unnecessary $\\max$}\\\\\n",
        "  & =\\gamma\\cdot ||v_1(s)-v_2(s)||_\\infty&&\\text{By definition of infinite norm}\\\\\n",
        "\\end{align*}\n",
        "\n",
        "Since $0\\leq \\gamma<1$, then the Bellman optimality operator is a contraction, and thus the iterative value function converges, by Banach fixed-point theorem."
      ]
    },
    {
      "metadata": {
        "id": "2un8layENUvV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question 2\n",
        "\n",
        "Note : As proposed by Pierre-Luc Beacon, we got inspiration from https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs-lecture.pdf \n",
        "\n",
        "## 1. Show that the values of two successive policies generated by policy iteration are nondecreasing.\n",
        "\n",
        "Let's define \n",
        "$$B V(s) = \\max_a \\sum_{s',r}P(s',r|s,a) [r+ \\gamma V(s')]$$\n",
        "and \n",
        "$$B^\\pi V(s) = \\sum_{s',r}P(s',r|s,\\pi(s)) [r+ \\gamma V(s')].$$\n",
        "\n",
        "The value function $V_i$ is the only fixed point of $B^\\pi$, obtained by evaluating the policy $\\pi_i$:\n",
        "$$V_i(s) = B^{\\pi_i} V_i = \\sum_{s',r}P(s',r|s,\\pi(s)) [r+ \\gamma V_i(s')]$$\n",
        "\n",
        "\n",
        "Given the policy iteration algorithm, we can see that the policy $\\pi_{i+1}$ is greedy with respect to $V_{i}$, since it is obtained as\n",
        "$$\\pi_{i+1}(s) = \\arg\\max_a  \\sum_{s',r}P(s',r|s,a) [r+ \\gamma V_{i}(s')].$$\n",
        "\n",
        "It means that\n",
        "$$B^{\\pi_{i+1}}V_{i} = B V_{i},$$\n",
        "by definition of $\\pi_i$. Furthermore, since $B^{\\pi_i}$ has only one fixed point, $V_i$, then we have\n",
        "$$B^{\\pi_i} V_i = V_i.$$\n",
        "\n",
        "Using these 2 equations, we have\n",
        "\n",
        "$$V_i = B^{\\pi_i}V_i \\leq B V_i = B^{\\pi_{i+1}}V_i ,$$\n",
        "(where the inequality sign arises since $BV_i$ is the optimal value function given $V_i$ and hence $B^{\\pi_i}V_i \\leq B V_i$). Using the same inequality multiple times, we get\n",
        "$$ V_i \\leq  B^{\\pi_{i+1}} V_i\\leq  (B^{\\pi_{i+1}})^2 V_i\\leq \\dots \\leq  (B^{\\pi_{i+1}})^n V_i. $$\n",
        "Taking the limit, since $B^{\\pi_{i+1}}$ has a unique fixed point $V_{i+1}$, we have\n",
        "$$ V_i \\leq \\lim_{n\\to\\infty} (B^{\\pi_{i+1}})^n V_i= V_{i+1}. $$\n",
        "Thus, the values of two successive policies generated by policy iteration are nondecreasing ($\\pi_i\\leq \\pi_{i+1}$) since, for any state $s\\in \\mathcal(S)$, $V_i(s)\\leq V_{i+1}(s)$ is true.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## 2. Assume a finite MDP and conclude (explain why) that policy iteration must terminate under a finite number of steps.\n",
        "\n",
        "In a finite MDP, since there is a finite set of actions and a finite set of states, then we only have a finite set of distinct policies to test. Since two successive policies are nondecreasing, then the policy iteration algorithm must halt under a finite number of steps (in a finite number of steps, we are guaranteed that there will be a stable policy between 2 successive iterations, because we cannot find an infinite number of strictly increasing policies when there is only a finite number of distinct policies available.)\n",
        "\n",
        "\n",
        "\n",
        "## 3. Finally, show that upon termination, policy iteration must have found an optimal policy (ie. one which satisfies the optimality equations).\n",
        "\n",
        "For the returned policy $\\pi_T$, we have that\n",
        "$$BV_T = B^{\\pi_T}V_T = V_T,$$\n",
        "where the first equality arises because the algorithm terminates when there is no change in the policy between 2 successive iterations (which means that $B^{\\pi_T}V_T$ is optimal, e.g. $BV_T = B^{\\pi_T}V_T$). The second equality is simply because, by definition, $V_T$ is a fixed point of $B^{\\pi_T}$. Finally, since we have\n",
        "$$BV_T = V_T,$$\n",
        "it means that $\\pi_T$, the returned policy, is optimal (because it satisfies the Bellman optimality equations, $B$).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "FG6_q4skOPVb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Track 1 "
      ]
    },
    {
      "metadata": {
        "id": "uUmB9wiKL3wd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Question 3\n",
        "\n",
        "# ------ MODEL DEFINITIONS -------\n",
        "# Storing (prob, rewards, next_state) for each state, action pair\n",
        "env_model_1 = {\n",
        "    'state_0': {\n",
        "        'action_0': [(0.5, 5, 'state_1'), (0.5, 5, 'state_0')],\n",
        "        'action_1': [(1, 10, 'state_1')],\n",
        "    },\n",
        "    'state_1': {\n",
        "        'action_2': [(1., -1, 'state_1')]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Simple Grid World\n",
        "#\n",
        "#  _  _  _  G           G = Goal (+1), W = Wall, P = Pit (-1)\n",
        "#  _  W  _  P\n",
        "#  _  _  _  _\n",
        "env_model_2 = {\n",
        "    'cell_0_0': {'UP': [(1., 0, 'cell_1_0')],\n",
        "                 'RIGHT': [(1., 0, 'cell_0_1')]},\n",
        "    'cell_0_1': {\"RIGHT\": [(1., 0., \"cell_0_2\")],\n",
        "                 \"LEFT\": [(1., 0., \"cell_0_0\")]},\n",
        "    'cell_0_2': {\"UP\" : [(1.0, 0., \"cell_1_2\")],\n",
        "                 \"RIGHT\": [(1.0, 0., \"cell_0_3\")],\n",
        "                 \"LEFT\": [(1.0, 0., \"cell_0_1\")]},\n",
        "    'cell_0_3': {\"UP\" : [(1.0, 0., \"pit\")],\n",
        "                 \"LEFT\": [(1.0, 0., \"cell_0_2\")]},\n",
        "    'cell_1_0': {\"UP\" : [(1.0, 0., \"cell_2_0\")],\n",
        "                 \"DOWN\": [(1.0, 0., \"cell_0_0\")]},\n",
        "    'cell_1_2': {\"UP\" : [(1.0, 0., \"cell_2_2\")],\n",
        "                 \"RIGHT\": [(1.0, 0., \"pit\")],\n",
        "                 \"DOWN\": [(1.0, 0., \"cell_0_2\")]},\n",
        "    'pit': {\"EXIT\":[(1.0, -1., \"terminal\")]},\n",
        "    \"terminal\":{\"\": [(1.,0., \"terminal\")]},\n",
        "    'cell_2_0': {\"RIGHT\": [(1.0, 0., \"cell_2_1\")],\n",
        "                 \"DOWN\": [(1.0, 0., \"cell_1_0\")]},\n",
        "    'cell_2_1': {\"RIGHT\": [(1.0, 0., \"cell_2_2\")],\n",
        "                \"LEFT\": [(1.0, 0., \"cell_2_0\")]},\n",
        "    'cell_2_2': {\"RIGHT\": [(1.0, 0., \"goal\")],\n",
        "                 \"LEFT\": [(1.0, 0., \"cell_2_1\")],\n",
        "                 \"DOWN\": [(1.0, 0., \"cell_1_2\")]},\n",
        "    'goal': {\"EXIT\" : [(1.0, +1., \"terminal\")]}\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QwgVnTuHMHKT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "99a886e0-c809-4e73-aaee-5874959019e4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517844482384,
          "user_tz": 300,
          "elapsed": 240,
          "user": {
            "displayName": "Philip Paquette",
            "photoUrl": "//lh6.googleusercontent.com/-DGrBTu2a2aw/AAAAAAAAAAI/AAAAAAAAANs/SZizZJUV1nA/s50-c-k-no/photo.jpg",
            "userId": "101375949079620784780"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "gamma = 0.95\n",
        "\n",
        "# ---------- GENERAL FUNCTIONS --------------\n",
        "def get_initial_policy(model):\n",
        "  \"\"\" Returns an initial policy that selects the first available action \"\"\"\n",
        "  return {state: list(model[state].keys())[0] for state in model}\n",
        "\n",
        "# ----------- 1) POLICY ITERATION ----------\n",
        "def policy_evaluation(model, policy):\n",
        "  \"\"\" Evaluates a policy and returns the value at each state \"\"\"\n",
        "  diff = float('inf')\n",
        "  values = {state: 0. for state in model}\n",
        "  \n",
        "  # Evaluating policy until convergence\n",
        "  while diff > 1e-5:\n",
        "    diff = 0\n",
        "    \n",
        "    # For each initial state, selecting action based on policy and re-evaluating\n",
        "    for state in model:\n",
        "      action = policy[state]\n",
        "      state_value = 0\n",
        "      for prob, reward, next_state in model[state][action]: \n",
        "        state_value += prob * (reward + gamma * values[next_state])\n",
        "      diff += abs(values[state] - state_value)\n",
        "      values[state] = state_value\n",
        "\n",
        "  # Returning values\n",
        "  return values\n",
        "\n",
        "def policy_improvement(model, policy, values):\n",
        "  \"\"\" Computes an improvement policy based on the last policy evaluation \"\"\"\n",
        "  new_policy = policy.copy()\n",
        "  \n",
        "  # Updating policy for each state\n",
        "  for state in model:\n",
        "    best_action_value = float('-inf')\n",
        "    \n",
        "    # Selecting action with largest value\n",
        "    for action in model[state]:\n",
        "      action_value = 0\n",
        "      for prob, reward, next_state in model[state][action]:\n",
        "        action_value += prob * (reward + gamma * values[next_state])\n",
        "      \n",
        "      # Keeping action if the current best\n",
        "      if action_value > best_action_value:\n",
        "        new_policy[state] = action\n",
        "        best_action_value = action_value\n",
        "  \n",
        "  # Returning new policy\n",
        "  return new_policy\n",
        "\n",
        "def policy_iteration(model):\n",
        "  \"\"\" Iteratives evaluates and improves a policy \"\"\"\n",
        "  old_policy = {}\n",
        "  new_policy = get_initial_policy(model)\n",
        "  \n",
        "  while old_policy != new_policy:\n",
        "    old_policy = new_policy\n",
        "    values = policy_evaluation(model, new_policy)\n",
        "    new_policy = policy_improvement(model, old_policy, values)\n",
        "\n",
        "  print(values)\n",
        "  return new_policy\n",
        "\n",
        "\n",
        "# Running on both models\n",
        "print(\"POLICY ITERATION\\n\")\n",
        "\n",
        "print('--- Running for model #1 ---')\n",
        "new_policy_1 = policy_iteration(env_model_1)\n",
        "print('Final Policy - Model 1', new_policy_1)\n",
        "print('')\n",
        "\n",
        "print('--- Running for model #2 (Grid World) ---')\n",
        "new_policy_2 = policy_iteration(env_model_2)\n",
        "print('Final Policy - Model 2', new_policy_2)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "POLICY ITERATION\n",
            "\n",
            "--- Running for model #1 ---\n",
            "{'state_0': -8.571333735771022, 'state_1': -19.99990516434245}\n",
            "Final Policy - Model 1 {'state_0': 'action_0', 'state_1': 'action_2'}\n",
            "\n",
            "--- Running for model #2 (Grid World) ---\n",
            "{'cell_0_0': 0.7737809374999999, 'cell_0_1': 0.8145062499999999, 'cell_0_2': 0.8573749999999999, 'cell_0_3': 0.8145062499999999, 'cell_1_0': 0.8145062499999999, 'cell_1_2': 0.9025, 'pit': -1.0, 'terminal': 0.0, 'cell_2_0': 0.8573749999999999, 'cell_2_1': 0.9025, 'cell_2_2': 0.95, 'goal': 1.0}\n",
            "Final Policy - Model 2 {'cell_0_0': 'UP', 'cell_0_1': 'RIGHT', 'cell_0_2': 'UP', 'cell_0_3': 'LEFT', 'cell_1_0': 'UP', 'cell_1_2': 'UP', 'pit': 'EXIT', 'terminal': '', 'cell_2_0': 'RIGHT', 'cell_2_1': 'RIGHT', 'cell_2_2': 'RIGHT', 'goal': 'EXIT'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_dP7o-qzM7EP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "57f0f26b-6f15-4ee9-a863-39c1dfd78971",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517844482800,
          "user_tz": 300,
          "elapsed": 228,
          "user": {
            "displayName": "Philip Paquette",
            "photoUrl": "//lh6.googleusercontent.com/-DGrBTu2a2aw/AAAAAAAAAAI/AAAAAAAAANs/SZizZJUV1nA/s50-c-k-no/photo.jpg",
            "userId": "101375949079620784780"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# ----------- 2) MODIFIED POLICY ITERATION ----------\n",
        "def modified_policy_evaluation_v1(model, policy, values):\n",
        "  \"\"\" Evaluates a policy using a 1-step backup \"\"\"\n",
        "\n",
        "  # Performing evaluation using 1-step value iteration\n",
        "  for state in model:\n",
        "    state_value = float('-inf')\n",
        "    for action in model[state]:\n",
        "      action_value = 0\n",
        "      for prob, reward, next_state in model[state][action]:\n",
        "        action_value += prob * (reward + gamma * values[next_state])\n",
        "      state_value = max(state_value, action_value)\n",
        "    values[state] = state_value\n",
        "\n",
        "  # Returning values\n",
        "  return values\n",
        "\n",
        "def modified_policy_evaluation_v2(model, policy, _, k=10):\n",
        "  \"\"\" Evaluates a policy and returns the value at each state \"\"\"\n",
        "  diff = float('inf')\n",
        "  values = {state: 0. for state in model}\n",
        "  step = 0\n",
        "  \n",
        "  # Evaluating policy for k steps\n",
        "  while step < k:\n",
        "    step += 1\n",
        "    \n",
        "    # For each initial state, selecting action based on policy and re-evaluating\n",
        "    for state in model:\n",
        "      action = policy[state]\n",
        "      state_value = 0\n",
        "      for prob, reward, next_state in model[state][action]: \n",
        "        state_value += prob * (reward + gamma * values[next_state])\n",
        "      diff += abs(values[state] - state_value)\n",
        "      values[state] = state_value\n",
        "\n",
        "  # Returning values\n",
        "  return values\n",
        "\n",
        "def modified_policy_iteration(model):\n",
        "  \"\"\" Performs modified policy iteration \"\"\"\n",
        "  old_policy = {}\n",
        "  new_policy = get_initial_policy(model)\n",
        "  old_values = {state: -1 for state in model}\n",
        "  values = {state: 0. for state in model}\n",
        "  \n",
        "  while sum([abs(values[state] - old_values[state]) for state in old_values]) > 1e-5:\n",
        "    old_values = values.copy()\n",
        "    old_policy = new_policy\n",
        "    values = modified_policy_evaluation_v2(model, new_policy, values)\n",
        "    new_policy = policy_improvement(model, old_policy, values)\n",
        "  print(values)\n",
        "  return new_policy\n",
        "\n",
        "# Running on both models\n",
        "\n",
        "print(\"MODIFIED POLICY ITERATION\\n\")\n",
        "\n",
        "print('--- Running for model #1 ---')\n",
        "new_policy_1 = modified_policy_iteration(env_model_1)\n",
        "print('Final Policy - Model 1', new_policy_1)\n",
        "print('')\n",
        "\n",
        "print('--- Running for model #2 (Grid World) ---')\n",
        "new_policy_2 = modified_policy_iteration(env_model_2)\n",
        "print('Final Policy - Model 2', new_policy_2)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MODIFIED POLICY ITERATION\n",
            "\n",
            "--- Running for model #1 ---\n",
            "{'state_0': 3.3966278814278654, 'state_1': -8.02526121523242}\n",
            "Final Policy - Model 1 {'state_0': 'action_0', 'state_1': 'action_2'}\n",
            "\n",
            "--- Running for model #2 (Grid World) ---\n",
            "{'cell_0_0': 0.7737809374999999, 'cell_0_1': 0.8145062499999999, 'cell_0_2': 0.8573749999999999, 'cell_0_3': 0.8145062499999999, 'cell_1_0': 0.8145062499999999, 'cell_1_2': 0.9025, 'pit': -1.0, 'terminal': 0.0, 'cell_2_0': 0.8573749999999999, 'cell_2_1': 0.9025, 'cell_2_2': 0.95, 'goal': 1.0}\n",
            "Final Policy - Model 2 {'cell_0_0': 'UP', 'cell_0_1': 'RIGHT', 'cell_0_2': 'UP', 'cell_0_3': 'LEFT', 'cell_1_0': 'UP', 'cell_1_2': 'UP', 'pit': 'EXIT', 'terminal': '', 'cell_2_0': 'RIGHT', 'cell_2_1': 'RIGHT', 'cell_2_2': 'RIGHT', 'goal': 'EXIT'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CcciCbPpL7Pl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "6d7a9e0d-cdba-4f60-cfd3-f1d736b75118",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517844483253,
          "user_tz": 300,
          "elapsed": 209,
          "user": {
            "displayName": "Philip Paquette",
            "photoUrl": "//lh6.googleusercontent.com/-DGrBTu2a2aw/AAAAAAAAAAI/AAAAAAAAANs/SZizZJUV1nA/s50-c-k-no/photo.jpg",
            "userId": "101375949079620784780"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# ----------- 3) VALUE ITERATION ----------\n",
        "def value_iteration(model):\n",
        "  \"\"\" Performs value iteration \"\"\"\n",
        "  policy = {}\n",
        "  diff = float('inf')\n",
        "  values = {state: 0 for state in model}\n",
        "  \n",
        "  # Performing value iteration (1-step backups)\n",
        "  while diff > 1e-5:\n",
        "    diff = 0\n",
        "    for state in model:\n",
        "      state_value = float('-inf')\n",
        "      for action in model[state]:\n",
        "        action_value = 0\n",
        "        for prob, reward, next_state in model[state][action]:\n",
        "          action_value += prob * (reward + gamma * values[next_state])\n",
        "        if action_value > state_value:\n",
        "          policy[state] = action\n",
        "          state_value = action_value\n",
        "      diff += abs(values[state] - state_value)\n",
        "      values[state] = state_value\n",
        "  \n",
        "  print(values)\n",
        "  return policy\n",
        "\n",
        "# Running on both models\n",
        "print(\"VALUE ITERATION\\n\")\n",
        "print('--- Running for model #1 ---')\n",
        "new_policy_1 = value_iteration(env_model_1)\n",
        "print('Final Policy - Model 1', new_policy_1)\n",
        "print('')\n",
        "\n",
        "print('--- Running for model #2 (Grid World) ---')\n",
        "new_policy_2 = value_iteration(env_model_2)\n",
        "print('Final Policy - Model 2', new_policy_2)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VALUE ITERATION\n",
            "\n",
            "--- Running for model #1 ---\n",
            "{'state_0': -8.571333735771022, 'state_1': -19.99990516434245}\n",
            "Final Policy - Model 1 {'state_0': 'action_0', 'state_1': 'action_2'}\n",
            "\n",
            "--- Running for model #2 (Grid World) ---\n",
            "{'cell_0_0': 0.7737809374999999, 'cell_0_1': 0.8145062499999999, 'cell_0_2': 0.8573749999999999, 'cell_0_3': 0.8145062499999999, 'cell_1_0': 0.8145062499999999, 'cell_1_2': 0.9025, 'pit': -1.0, 'terminal': 0.0, 'cell_2_0': 0.8573749999999999, 'cell_2_1': 0.9025, 'cell_2_2': 0.95, 'goal': 1.0}\n",
            "Final Policy - Model 2 {'cell_0_0': 'UP', 'cell_0_1': 'RIGHT', 'cell_0_2': 'UP', 'cell_0_3': 'LEFT', 'cell_1_0': 'UP', 'cell_1_2': 'UP', 'pit': 'EXIT', 'terminal': '', 'cell_2_0': 'RIGHT', 'cell_2_1': 'RIGHT', 'cell_2_2': 'RIGHT', 'goal': 'EXIT'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EvD3akc5NuVh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Explain and explore how the convergence rate of these algorithms is affected by the discount factor.\n",
        "\n",
        "- We were able to experiment by varying the discount factor. We found out that a discount factor closer to 1 needs a longer time to converge, because each state needs to take into account the discounted reward of a greater number of future states to reach its accurate value.\n",
        "- Alternatively, a discount factor closer to 0 will lead to faster convergence, because each state only needs to take into account the discounted reward of a few future states."
      ]
    }
  ]
}